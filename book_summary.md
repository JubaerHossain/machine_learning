# Machine Learning from scratch

> Artificial intelligence requires something called artificial reasoning,
otherwise known as machine reasoning. When humans learn new things and
draw conclusions, we go through a process known as inductive reasoning.
We take pieces of information to draw new conclusions. Usually, there is no
set rule that we are taught to go by. We learn from experience and draw our
own rules by cumulative experience. For example, I could tell you that it
snowed 15 times last December. Therefore, it will snow this December again.
Every day in January was cold, so every day this January will be cold. So, I
should bring a jacket.

> Humans think differently from machines because we don’t interpret
numerical data patterns. We learn from positive and negative rewards and
from the feelings we experience in our daily lives. Getting a computer to use
inductive reasoning will get us closer to having ‘human-like’ machines
So, for computers to learn, they need to have data to learn from. Data
usually needs to be numerical, so that it can be interpreted by mathematical
models and algorithms. If we give a computer enough data, it will create the
parameters to design its own model or algorithm, to predict new situations
based on prior experience. This is the basis of machine learning. Feed the
computer experience so that it can predict new outcomes in the future
through inductive reasoning.

## Key term: Inductive reasoning. 
Drawing on information from experiences and our environment to draw generalizations.
The ability to see the difference between these pictures based on our knowledge of cats and dogs; this is what we know as reasoning. The goal of
artificial intelligence is to teach computers how to have similar abilities to human-like reasonin Translation apps must take our voice or our text and analyze the sentence structure to make
meaning.

> Data science is more of a general term, whereas machine learning is a part of data science.


Data science is the management and analysis of data, and within data
science, there are many ways to analyze the data and use it to learn. Machine
learning is its own field of study within the realm of computer science. The
idea is to predict something, and as you add more data, compare predictions
to actual outputs. Over time, your ability to predict should improve, and
errors will be reduced.

One of the most important functions of the human brain is the ability to
change our behavior based on the outcomes of past events and situations. If a
situation has a positive outcome, we remember that, and in turn, if a situation
has a negative outcome, we also store that in our memory. Later, we’ll use
this ‘data’ to make decisions about new situations. Over time, we learn how

to interpret situations, even if they are totally new, and we aren’t totally sure
how to behave or respond.


Machine learning helps us create a mathematical way of copying the
human ability to learn over time and through a new experience. Machine
learning models learn over time and improve their methods of predictions,
therefore improving outcome. Past data is compiled, and over time, the model
can make better and more accurate predictions. Over time, the program will
be able to make more accurate predictions because of the new data it’s been
given. It learns over time how to do a better job at completing a given task.


Jobs that used to require human thinking can now be done with machine
learning. Factories, medical diagnosis, and even taxis have the potential to be
done using artificial intelligence and machine learning. Data is becoming an
ever more important field of study. Patterns in data can be impossible to
interpret with a human brain, which is why we use machines to detect
patterns.


## rote learning

Sandford University refers to **machine learning** as `"The science of getting computers to act without being explicitly programmed."`

Normal computer programming uses an input command to direct the
model. An example of this would be plugging 4+4 into your python window.
It will give you the answer, 8. Instead, machine learning uses what's called
input data. Input data is what the machine needs to learn, whereas input
command would mean that the machine is not self-learning. The programmer
doesn't specify what he wants the answer to be. Instead, the machine
interprets the data on its own, which makes it self-learning. In machine
learning, the machine takes data over time and uses it to create a new model
for something.

The machine will detect patterns and structure within data, based totally in
statistical logic. It is completely based on mathematical algorithms. Rather
than using intuition to search for patterns, patterns our found at a quantitative
and logical level by the machine learning model. The more relevant data a
machine is exposed to, the better it understands and can predict the outcome
of the model.


Before you start, you should have a good amount of knowledge of the data
you are choosing. Where did it come from, and how was it collected? What
format is it in, and what will the challenges be while interpreting it? These
are the types of questions you should be asking when you begin


## Statistics


Statistics make up the core of machine learning. If you aren’t willing to
dive into statistics, then machine learning isn’t for you. Machine learning
uses statistical algorithms to help computers learn. Machine learning is all
about the tracking of data and how computers can use data to improve
themselves.



> There are two types of statistics which are relevant to this book. The first
one is *descriptive analysis*, which you might use during the beginning of your
modeling process to look for indicators in your data. But most of what we do
in machine learning falls into a different category called predictive analysis.
Key term; Descriptive analysis. The descriptive analysis helps us
examine we are right now. Looking at our current situation in context to the
past and seeing why things are the way they are. Why do some things sell
better than others? What trends are we seeing in products currently on the
market?

## Key term; Predictive analysis. 
Predictive analysis helps us to see and
understand what will happen in the future based on indicators that are
currently present. When we are using machine learning for predictive
analysis, it’s important for us to stay current and continue to feed the model
new data. What trends should we be on the lookout for?
Machine learning is just another way to understand the data that is around
us and to help us understand our present and predict the future. But it requires
data from the past and present so that we can find trends and see where they
might lead.



Within statistics, there are two over-arching categories of data that we will
use, and all our data will fall into one category or the other somehow.
The first category is quantitative data. Quantitative data is data that can
be measured with a numerical value. Some examples of quantitative data
include height, income, or the square footage of a house. All these variables
can be measured by some number, which makes them quantitative


The second category is qualitative data. Qualitative data is data where the
variables are assigned to categories or classes. Examples of qualitative data
would include someone’s gender, blood type, or whether a piece of real estate
has a pool or not. This data can be sorted by its identity and is non-numerical.
Therefore it is qualitative.


**Quantitative** data can either be discrete or continuous. If we have a data set
where there is a variable recording the number of patients that a hospital had
last year, this would be considered discrete. Discrete variables have a finite
amount of values that they can have, and they are always whole numbers. It
would be impossible to have half a patient or a percentage of a patient.
Therefore this variable is discrete.


Data can also be continuous. An example of continuous data would be a
variable for income. Income can take on half values, and there is a virtually
infinite amount of possibilities for the value of income in data.

Some other important terms to remember are the mean, median, and mode.


## regressions

The mean is our average value for data. If we have a variable for a
person’s age, we will find the mean of age by adding all the ages together and
then dividing by the number of respondents in a data set.

The median is the value in the middle of the dataset. If you took all the
responses for age and found the response that was in the exact middle of a
sorted list of responses, then this would be your median.

The mode is the response that occurs the most frequently. If we took a
sample of eleven people’s ages and found that the ages were 19, 19, 20, 21,
22, 22, 22, 23, 24, 24, 25 then the mode would be 22, because it occurs the
most frequently in this sample. The median would also be 22 because it
happens to be in the middle of this sorted list of responses.

When you are making a statistical model, there are many important terms
that have to do with the accuracy of our models. The most important, and the
most frequently mentioned in this book are bias and variance. 

These are different kinds of prediction errors that can occur when we are creating
statistic models.

Ideally, we’d like to minimize the prevalence of bias and
variance in our models. They will always be present, and as a data scientist,
you will have to find the right balance of bias and variance in your models,
whether that’s by choosing different data or using different types of models.
There are many ways to reduce variance and bias within a model, dependent
on what you are trying to do with the data. By trying to reduce these with the
wrong approach, you run the risk of overfitting or underfitting your model.
When your model is bias, it means that the average difference between your
predictions and the actual values is very high.

Variance is how to spread out our predicted data points are. Usually, the
variance is a result of overfitting to the sample data we used to create the
model. It doesn't do very well at predicting the outcome of new variables.

There will always be some level of error in your models. It’s a fact of life
that no matter how good you are at predicting something, there is always
some random or nonrandom variation in the universe that will make your
prediction slightly off from the true outcome.


If you use too many independent variables, this can also be a cause
of the high variance. Sometimes, if the variance is too high, we can combat
that by allowing a small amount of bias in the model. This is known as
regularization

In statistics, the population is the group of people or the set of data you
are trying to analyze. The sample is the subgroup of that population, whose
data you use to create your model. The parameters are the characteristic of
the variables of the population that you are trying to identify and make
predictions from in your model.


Descriptive statistics is the use of data to examine a population. Typically,
descriptive statistics involve the mean or average, mode, media, size,
correlation. Machine learning falls into the category of inferential statistics
because we are using the data to find patterns and relations but also to make
predictions based on this information. Inferential statistics, or descriptive
stats, is using the characteristics of your population to make predictions. This
is where your regression models and classification models will come in.
When we infer something, we make a logical deduction about a populationbased and the knowledge we are given.


We use our variance to find the standard deviation. Standard deviation is
the average of the distances between the predicted data points and the real
data points on a regression or prediction model.


We must also be sure to be aware of models that suffer from overfitting
and underfitting. An overfitted model is good at predicting outcomes using
the training data, but when you introduce new data, then it struggles. It’s like
a model that memorizes instead of learns. It can happen if you don’t use
random data in your training sample.

Underfitting describes a model that is too simple, and it doesn't examine
any significant data patterns. It may do a good job of predicting, but the
variables and parameters aren't specific enough to give us any meaningful
insights if you don't have enough training data, your model could be under
fitted.


One of the most commonly made mistakes when people are looking at
data, is confusing correlation with causation. If I told you that every person
who committed a murder last year bough eggs every week, I couldn’t claim
that people who buy eggs are murderers. Maybe looking at my data, I see a
rise in people buying milk, as well as a rise in teen pregnancy. Would I be
able to claim that there is an association between people drinking a lot of
milk and teen pregnancy? Or teenagers getting pregnant caused people to
buy more milk.

This is the difference between correlation and causation. Sometimes the
data shows trends that seem like they are related. When two events are
correlated, it means that they seem to have a relationship because they move
along the graph at a similar trajectory, and during a similar space in time.
Whereas causation means that the relationship between the two events
involves one event causing another.


## Choosing the right kind of model for machine learning

Statistical algorithms can serve several purposes. Some predict a value;
like a regression model that predicts your income base on your years of
education and work experience. Some models predict the likelihood of an
event occurring, like a medical model that predicts the likelihood of a patient
surviving a year, or two years, etc. Other models sort things by placing them
into different categories or classes, like a photo recognition software sorting
photo of different types of dogs.

Depending on the outcome you are looking for, you will need to have your
statistical toolbelt. You need to familiarize yourself with the technical skills
of statistics. Also, you will have to know which tool you want to use, and
when to use it. Here I have created a comprehensive list of the different kinds
of statistical models that are very common in machine learning. 


>> Different kinds of statistical models that are very common in machine learning


For machine learning to be effective, you must choose the right model and
the model that works best and have relevant data for the model and the
question at hand.

The better you are at interpreting your data, the more easily you will be
able to identify trends and patterns so that you can anticipate the next change.

Machine learning can be broken down into three different categories, each
one containing several unique algorithms that serve different purposes. To
start, we’ll talk about the differences between supervised, unsupervised, and
reinforcement learning.


1. Supervised learning
2. Unsupervised learning
3. Reinforcement learning

## 1. Supervised learning::
In supervised learning, programmers use labeled data. Before we begin
using the algorithms, the data that we are looking at is already predetermined.
We know the inputs and outputs we are looking for. X, and Y. We are trying
to find a relationship between X and Y that we have chosen.


After you find a relationship between X and Y, you get a model, which
will predict an outcome based on those relationships that your machine has
observed in the data. Supervised learning is used for regression and
classification models. In machine learning, we refer to features as a certain
measurable property or characteristic of the data.

The first type of supervised learning that we'll talk about and the first type
of statistical model is called a regression. Regression is a model where the
data input and output are continuous. There are different types of regression,
but the most basic form is linear regression. We use linear regression to find a
relationship between some input X and an output Y.

Once we have estimated this relationship, we can predict Y with X. Linear models can, and usually do, have more than one X. In regression; output Y has a numerical value.

## Regression Analysis
Regression is the simplest type of machine learning; this is usually where
you start when you are first learning how to use your data. You have a set of
X values, and you want to study their relationship with Y, the output. Our
independent variables, the Xs in our model, are given weight, and for each
value of X, it is multiplied by weight until the aggregate function creates a
prediction for Y.


We can create a predictive model for Y by using data where we already
know the X and Y. Regressing this information will give us the weights of X.
If we have enough relevant data, eventually we will be able to predict and
unknown Y with known values for X.

### scatterplot
We graph our known Y and X values on a scatterplot, and our regression
model finds the “best fit” line through the data points. The regression line is
called a hyperplane. The steepness of the line is called the slope.

### Deviation
We can measure the distance between the predicted value and actual data
point, and we call that measurement deviation.

Our goal when we create a linear regression is the minimize the deviation in our predictions. The smaller that difference, the deviation is, the more accurate your model is.


Most of the statistical models used in machine learning are rooted in this first algorithm.


Creating a model that will predict an outcome by plotting our data points along a line or in clusters. But the line isn’t always straight, and sometimes the line doesn’t show us the best fit.

### Sigmoid function
An example of a regression function that is nonlinear is the Sigmoid function. The Sigmoid function creates an S-shaped curve. Instead of predicting a value, the Sigmoid function takes independent variables and produces a probability between one and zero.

## Simple Linear Regression

In simple linear regression, we study the relationship between some predicted value Y and some predictor X. We call our Y the dependent variable because it is dependent on the value of X. Our
X is known as the independent variable.


If you took algebra or pre-calculus in high school or college then you might remember the equation of a line was;

> `Y = mX + b`


we will be building off it from here on out.

Most of the statistical analysis involves a plot like the one pictured above, which makes
some prediction for an output dependent variable based on an input, the independent variable. This is an example of supervised learning because we are specifying the Y variable and the X variable that we are using before we start modeling.


With almost all predictions, there will be more than one independent variable that will determine our dependent variable. This leads us to our next type of regression.

## Multiple Linear Regression
In data science and most tasks in statistics, this will be the most popular type of regression. In multiple linear regression, we will have one output variable Y, just like before. The difference now though, is that we will have multiple Xs or independent variables that will predict our Y.


An example of using multiple linear regression to predict the price of
apartments in New York City real estate. Our Y or dependent variable is the
price of a New York City apartment. The price will be determined by X, our
independent variables such as the square footage, distance to transportation,
number of rooms. 

If we were to write this out as an expression it would look something like:
apt_price = β0 + β1 sq_foot + β2 dist_transport + β3 num_rooms

> `Y = b + m1X1 + m2x2 + m3x3`

Ordinary Least Squares OLS will try to find a regression line that
minimizes the sum of errors squared

## Polynomial Regression
In polynomial regressions, our model will result in a line that has a curve

If we tried to use linear regression to fit a graph that has nonlinear
characteristics, we would do a poor job of creating the best fit line. Take the
graph on the left, for example; the scatter plot has an upward trend like
before, but with a curve. In this case, a straight line doesn’t work. Instead,
with a polynomial regression, we will create a line with a curve to match the
curve in our data, like the graph on the right

The equation of a polynomial will look like the linear equation, with the
difference being that there will be some polynomial expression attached to
one or more of our X values. For example:

Y = mX2+b


## Support Vector Regression
This is another important tool for data scientists and one that you should familiarize yourself with. It is most commonly used in case classification. The idea here is to find a line through a
space that separates data points into different classes. Support Vector Regression is another type of supervised learning. Its also used for regression analysis. It is a type of binary classification technique not related to probability.

In support of Vector Regression, all your training data falls into one category or the other.

You want to find out which category a new data point falls into. Your data is separated into these two classes by a hyperplane.

When you're creating a model for the hyperplane, you are trying to find a hyperplane that maximizes the distance between the two classes.

The wider the margin is, the better.


### Ridge Regression
This is a technique commonly used to analyze data that
suffer from multicollinearity. Using ridge regression properly can reduce
standard errors and make your model more accurate, depending on the
characteristics of your data.

Ridge regression can be useful when your data contains independent
variables with a high correlation. If you could make a prediction about an
independent variable by using another independent variable, then your model
is at risk of multicollinearity. For example, if you are using variables that
measure a person’s height and weight; these variables are likely to create
multicollinearity in the model.

Another method is to improve the model's accuracy by standardizing
independent variables. The simplest way is to change the coefficients of some
independent variables to zero to reduce complexity.


### LASSO Regression

LASSO regression is another ‘shrinkage’ technique.
A very similar approach to ridge regression in that it encourages leaner,
simpler models for prediction. In lasso regression, the model is a little more
stringent about reducing the value of coefficients. LASSO stands for the least
absolute shrinkage and selection operator.

### ElasticNet Regression. 
ElasticNet regression works by combining the
techniques of LASSO and ridge regression. Its main goal is to attempt to
improve upon LASSO regression. It is a combination of both the methods of
rewarding lower coefficients in LASSO and Ridge regression. All three of
these models can be accessed in the glmnet package in R and Python.


### Bayesian Regression. 
Bayesian regression models are helpful when we have insufficient data or data with poor distribution.

In Bayesian regression, the dependent variable Y is not a value but a
probability. Rather than trying to predict a value, we are trying to predict the
probability of an outcome. This is known as frequentist statistics, and Bayes
theorem makes up the foundation for this type of statistics. Frequentist
statistics hypothesize whether something will happen, and the probability that
it will happen.

## Decision trees
Decision trees break data into subcategories using decision and leaf nodes in the shape of a
tree.

Decision trees have a few advantages over neural networks (discussed later
in this chapter). For one thing, neural networks require huge amounts of data
and powerful computers in order to process them. The upside to using a
decision tree is that they are relatively simple, especially when you compare
them to neural networks.

Decision trees are another form of supervised learning, meaning that we
label the categories that we want to sort before creating the model. In some
instances, decision trees can complete regression tasks, but typically they are
used as classification models. When decision trees are used for regression,
the leaf nodes end in probabilities.

Decision trees start with what’s called a root node at the top of the tree.
Then the root node is split into two nodes after the root node. Nodes are
individual leaves on the tree, and the middle nodes are where the decisions
are made, known as the decision nodes. The decision tree ends at the bottom
in what’s called a terminal node is at the bottom of a branch, where the
decision is complete.

Like neural networks,
decision trees often suffer from overfitting. A decision tree usually won't
work with other sets of data because the sorting is so specific to each dataset.


## Random forests
If a real forest is made up of several different trees, then that’s exactly
what a random forest is. Instead of just having one decision tree, you split the
data into several decision trees. When you only have one tree, models can
often suffer from high variance. Creating a random forest is a way to combat
that in your model. It’s one of the best tools available for data mining. A
random forest is as close as you can get to a pre-packaged algorithm for data
mining purposes.

In a random forest, all the trees work together. The aggregate result of all
the trees is usually right, even if a few trees end up with bad predictions. To
create the final prediction, the results of all the trees are tallied. Using votes
from the average values of all the trees gives us a final prediction.




## Introduction to Mathmatical Thinking

* https://www.youtube.com/watch?v=LN7cCW1rSsI


> According to this description, the mathematician identifies and analyzes
abstract patterns—numerical patterns, patterns of shape, patterns of motion, patterns of behavior, voting patterns in a population, patterns of repeating chance events, and so on. Those patterns can be

either real or imagined, visual or mental, static or dynamic, qualitative or quantitative, utilitarian or
recreational. They can arise from the world around us, from the pursuit of science, or from the inner
workings of the human mind. Different kinds of patterns give rise to different branches of mathematics

### Arithmetic and number theory study the patterns of number and counting.
* Geometry studies the patterns of shape.
* Calculus allows us to handle patterns of motion.
* Logic studies patterns of reasoning.
* Probability theory deals with patterns of chance.
* Topology studies patterns of closeness and position.
* Fractal geometry studies the self-similarity found in the natural world.

> algebraic expressions, complicated-looking formulas, and geometric diagrams

When two numbers are added, their order is not important.
However, it is usually written in the symbolic form

> `m + n = n + m`

The same is true for mathematics; the symbols on a page are just a
representation of the mathematics.

When read by a competent performer (in this case, someone trained
in mathematics), the symbols on the printed page come alive—the mathematics lives and breathes in the
mind of the reader like some abstract symphony.

> Mathematics makes the invisible visible

And it sets the scene for this book, the main aim of which is to provide you with the basic mental tools you will need to enter this new world of modern mathematics 


> There are so many different mathematical techniques, with new ones being developed all the time, that it is impossible to cover them all in K-16 education. By the time a college frosh graduates and enters the workforce, many of the specific techniques learned in those four college-years are likely to be no longer as important, while new ones are all the rage. The educational focus has to be on learning how to learn.
